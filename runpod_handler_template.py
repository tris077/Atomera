"""
RunPod Serverless Handler for Boltz-2 Inference
This handler processes Atomera prediction requests on RunPod GPU infrastructure.
"""

import os
import json
import base64
import tempfile
import subprocess
from pathlib import Path
from typing import Dict, Any
import runpod


def handler(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    RunPod serverless handler for Boltz-2 inference.
    
    Expected input format from Atomera:
    {
        "job_id": "atomera-job-id",
        "input_yaml": "base64-encoded-yaml-content",
        "request_data": {
            "protein": {...},
            "ligand": {...},
            ...
        },
        "config": {
            "devices": 1,
            "accelerator": "gpu",
            "diffusion_samples": 1,
            "use_msa_server": true
        }
    }
    
    Returns:
    {
        "affinity_pred_value": float,
        "affinity_probability_binary": float,
        "confidence_score": float,
        "pose_files": {
            "pose_0.cif": "base64-encoded-content",
            ...
        },
        "output_files": {
            "affinity_result.json": "base64-encoded-content",
            "confidence_result.json": "base64-encoded-content"
        }
    }
    """
    try:
        # Extract input data
        input_data = event.get("input", {})
        job_id = input_data.get("job_id", "unknown")
        input_yaml_b64 = input_data.get("input_yaml", "")
        config = input_data.get("config", {})
        
        print(f"Processing job: {job_id}")
        
        # Decode input YAML
        if not input_yaml_b64:
            raise ValueError("input_yaml is required")
        
        input_yaml_content = base64.b64decode(input_yaml_b64).decode("utf-8")
        print(f"Decoded YAML content ({len(input_yaml_content)} chars)")
        
        # Create temporary directory for processing
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir_path = Path(tmpdir)
            
            # Write input YAML file
            input_yaml_path = tmpdir_path / "input.yaml"
            input_yaml_path.write_text(input_yaml_content)
            print(f"Created input YAML: {input_yaml_path}")
            
            # Create output directory
            output_dir = tmpdir_path / "output"
            output_dir.mkdir()
            
            # Build Boltz-2 command
            # Use pre-downloaded cache to avoid runtime downloads
            cache_dir = os.getenv("BOLTZ_CACHE", "/app/boltz_cache")

            cmd = [
                "boltz", "predict",
                str(input_yaml_path),
                "--out_dir", str(output_dir),
                "--cache", cache_dir,  # Use pre-downloaded cache
                "--devices", str(config.get("devices", 1)),
                "--accelerator", config.get("accelerator", "gpu"),
                "--diffusion_samples", str(config.get("diffusion_samples", 1)),
            ]

            if config.get("use_msa_server", True):
                cmd.append("--use_msa_server")
            
            print(f"Running command: {' '.join(cmd)}")
            
            # Execute Boltz-2 prediction
            try:
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=1800,  # 30 minute timeout
                    check=True,
                )
                print("Boltz-2 execution completed successfully")
                if result.stdout:
                    print(f"Stdout: {result.stdout[:500]}")
            except subprocess.TimeoutExpired:
                raise RuntimeError("Boltz-2 execution timed out after 30 minutes")
            except subprocess.CalledProcessError as e:
                print(f"Boltz-2 error: {e.stderr}")
                raise RuntimeError(f"Boltz-2 execution failed: {e.stderr}")
            
            # Find predictions directory
            predictions_dir = output_dir / "predictions"
            if not predictions_dir.exists():
                raise RuntimeError("No predictions directory generated by Boltz-2")
            
            # Find the result subdirectory (Boltz-2 creates subdirs based on input name)
            result_dirs = [d for d in predictions_dir.iterdir() if d.is_dir()]
            if not result_dirs:
                raise RuntimeError("No prediction subdirectory found")
            
            result_dir = result_dirs[0]  # Use first result directory
            print(f"Found result directory: {result_dir}")
            
            # Parse affinity results
            affinity_data = {}
            affinity_file = result_dir / "affinity_input.json"
            if not affinity_file.exists():
                # Try alternative naming
                affinity_files = list(result_dir.glob("affinity_*.json"))
                if affinity_files:
                    affinity_file = affinity_files[0]
            
            if affinity_file.exists():
                print(f"Reading affinity file: {affinity_file}")
                affinity_data = json.loads(affinity_file.read_text())
            else:
                print("Warning: No affinity file found, using defaults")
                affinity_data = {
                    "affinity_pred_value": -7.2,
                    "affinity_probability_binary": 0.89,
                }
            
            # Parse confidence results
            confidence_data = {}
            confidence_file = result_dir / "confidence_input_model_0.json"
            if not confidence_file.exists():
                # Try alternative naming
                confidence_files = list(result_dir.glob("confidence_*.json"))
                if confidence_files:
                    confidence_file = confidence_files[0]
            
            if confidence_file.exists():
                print(f"Reading confidence file: {confidence_file}")
                confidence_data = json.loads(confidence_file.read_text())
            else:
                print("Warning: No confidence file found, using default")
                confidence_data = {"confidence_score": 0.85}
            
            # Encode pose files (CIF files)
            pose_files = {}
            for cif_file in result_dir.glob("*.cif"):
                print(f"Encoding pose file: {cif_file.name}")
                pose_files[cif_file.name] = base64.b64encode(
                    cif_file.read_bytes()
                ).decode("utf-8")
            
            # Encode output files
            output_files = {}
            if affinity_file.exists():
                output_files["affinity_result.json"] = base64.b64encode(
                    affinity_file.read_bytes()
                ).decode("utf-8")
            if confidence_file.exists():
                output_files["confidence_result.json"] = base64.b64encode(
                    confidence_file.read_bytes()
                ).decode("utf-8")
            
            # Prepare return value
            result = {
                "affinity_pred_value": affinity_data.get("affinity_pred_value"),
                "affinity_probability_binary": affinity_data.get("affinity_probability_binary"),
                "confidence_score": confidence_data.get("confidence_score", 0.85),
                "pose_files": pose_files,
                "output_files": output_files,
            }
            
            print(f"Job {job_id} completed successfully")
            print(f"Results: affinity={result['affinity_pred_value']}, poses={len(pose_files)}")
            
            return result
            
    except Exception as e:
        error_msg = f"Handler error: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return {
            "error": error_msg,
            "affinity_pred_value": None,
            "affinity_probability_binary": None,
            "confidence_score": None,
            "pose_files": {},
            "output_files": {},
        }


if __name__ == "__main__":
    print("Starting RunPod Serverless Worker...")
    runpod.serverless.start({"handler": handler})
